# Databricks notebook source
# MAGIC %md
# MAGIC # 参数调优

# COMMAND ----------

# MAGIC %md
# MAGIC ## 调参的基本思想
# MAGIC 
# MAGIC 机器学习的调参探究比较少
# MAGIC * 调参的方式总是根据数据的状况而定，所以没有办法一概而论
# MAGIC * 大家也没有特别好的办法
# MAGIC 
# MAGIC 通过画学习曲线，或者网格搜索，我们能够探索到调参边缘（代价可能是训练一次模型需要跑好几天），但是在现实中，高手调参恐怕还是多依赖于经验，而这些经验来源于：
# MAGIC * 非常正确的调参思路和方法
# MAGIC * 对模型评估指标有深刻的理解
# MAGIC * 对数据的感觉和经验
# MAGIC * 用洪荒之力去不断的尝试
# MAGIC 
# MAGIC 我们也许无法学到高手们多年积累的经验，但我们可以学习他们对模型评估指标的理解和调参思路。
# MAGIC 
# MAGIC 模型调参，第一步是要找准目标：我们要做什么？一般来说，这个目标是提升某个模型评估指标，比如对随机森林来说，我们想要提升的是模型在未知数据上的准确率（由score或oob_score_来衡量）。找准了这个目标，我们就需要思考：模型在未知数据上的准确率受什么因素影响？在机器学习中，我们用来**衡量模型在未知数据上的准确率**的指标，叫做**泛化误差（Genelization error）**。

# COMMAND ----------

# MAGIC %md
# MAGIC ### 泛化误差
# MAGIC 
# MAGIC 当模型在未知数据（测试集或者袋外数据）上的表现糟糕时，我们说模型的泛化程度不够，泛化误差大，模型的效果不好。泛化误差受到模型的结构（复杂度）影响。当模型太复杂，模型就会过拟合，泛化能力就不够，所以泛化误差大。当模型太简单，模型就会欠拟合，拟合能力就不够，所以误差也会大。只有当模型的复杂度刚刚好的才能够达到泛化误差最小的目标。

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC 那模型的复杂度与我们的参数有什么关系呢？
# MAGIC 
# MAGIC 对模型来说，树越茂盛，深度越深，枝叶也越多，模型就越复杂。所以树的模型是天生复杂度高的模型，随机森林是以树模型为基础，所以随机森林也是天生复杂度高的模型。随机森林的参数，都是向着一个目标去：减少模型的复杂度，把模型往复杂度低的方向调整，防止过拟合。当然了，调参没有绝对，也有天生复杂度低的随机森林，所以调参之前，我们需要先判断，模型现在究竟是什么样的一个复杂度。

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC 泛化误差的背后其实是“偏差-方差困境”，原理十分复杂。我们只需要记住四点：
# MAGIC 1. 模型太复杂或者太简单，都会让泛化误差高，我们追求的是位于中间的平衡点
# MAGIC 2. 模型太复杂就会过拟合，模型太简单就会欠拟合
# MAGIC 3. 对树模型和树的集成模型来说，树的深度越深，枝叶越多，模型越复杂
# MAGIC 4. 树模型和树的集成模型的目标，都是减少模型复杂度

# COMMAND ----------

# MAGIC %md
# MAGIC ## 随机森林的调参
# MAGIC 
# MAGIC 随机森林的调参方向：降低复杂度。
# MAGIC 
# MAGIC 而欧美可以将那些对复杂度影响巨大的参数挑选出来，研究他们的单调性，然后专注调整那些能最大限度让复杂度降低的参数。对于那些不单调的参数，或者反而会让复杂度升高的参数，我们就视情况使用，大多数时候可以退避。
# MAGIC 
# MAGIC | 参数 | 对模型在未知数据上的评估性能的影响 | 影响程度 |
# MAGIC | ----- | ----- | ----- |
# MAGIC | n_estimators | 提升至平稳，n_estimators增大，不影响单个模型的复杂度 | ★★★★ |
# MAGIC | max_depth | 有增有减，默认最大深度，即最高复杂度，向复杂度降低的方向调参，max_depth减小，模型更简单，复杂度降低 | ★★★ |
# MAGIC | min_samples_leaf | 有增有减，默认最小限制1，即最高复杂度，向复杂度降低的方向调参，min_samples_leaf增大，模型更简单，复杂度降低 | ★★ |
# MAGIC | min_samples_split | 有增有减，默认最小限制2，即最高复杂度，向复杂度降低的方向调参，min_samples_split增大，模型更简单，复杂度降低 | ★★ |
# MAGIC | max_features | 有增有减，默认auto，是特征总数开平方，位于中间复杂度，既可以向复杂度升高的方向，也可以向复杂度降低的方向调参。<br />max_features减小，模型更简单   <br />max_features增大，模型更复杂<br />max_features是唯一的，既能够让模型更简单，也能够让模型更复杂的参数，所以在调整这个参数的时候，需要考虑我们的调参的方向 | ★ |
# MAGIC | criterion | 有增有减，一般使用gini | 看具体情况 |

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC 有了以上的知识储备，我们现在也能够通过参数的变化来了解，模型什么时候达到了极限，当复杂度已经不能再降低的时候，我们就不必再调整了，因为调整大型数据的参数是一件非常费时费力的事。除了学习曲线和网格搜索，我们现在有了基于对模型和正确的调参思路的“推测”能力，这能够让我们的调参能力更上一层楼。
